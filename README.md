# Tupa-AraSci 
## Multihead Encoders to Multihead Attention decoder. (Novel architecture)

The last two methods involved a 20 fixed length vector composed by concatenating the faraday cup and magnetometer data. In the method discussed here, we use 2 different encoders for each data type, both having very deep architectures with multiheaded attention incorporated. At the end, both outputs of encoders are stacked into one fixed length vector that is processed by a deep neural network to the prediction. A novel architecture that involves merging both architecture types, the Recurrent Neural Networks and Transformers.

[Explanatory video]()
